{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpuVtwsArGq5AjoN4dj79n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9ae745755b44a2daf406c50d3161d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3801450651da496fa5c5d3503d6a0cb9",
              "IPY_MODEL_c7e57748afca439696ca942234070afd",
              "IPY_MODEL_52ab3e64486c4ede84c6e15528ec9c12"
            ],
            "layout": "IPY_MODEL_abd93eaeb1e94437a184cbf57aed5b9c"
          }
        },
        "3801450651da496fa5c5d3503d6a0cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a97462338b7482e8478fa40dc4f622f",
            "placeholder": "​",
            "style": "IPY_MODEL_f9cef04c59fc4f7ca5ddfc4c1565d3be",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7e57748afca439696ca942234070afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d09b8a3c8b4e63838cd3c489766a7a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6fd76d4fba649b2a993ee16939b95ae",
            "value": 3
          }
        },
        "52ab3e64486c4ede84c6e15528ec9c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b55d4aa373dd40ab8b66991d6ad8f811",
            "placeholder": "​",
            "style": "IPY_MODEL_0c9107b6955340c69a338ce950e4e6ac",
            "value": " 3/3 [01:14&lt;00:00, 24.56s/it]"
          }
        },
        "abd93eaeb1e94437a184cbf57aed5b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a97462338b7482e8478fa40dc4f622f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9cef04c59fc4f7ca5ddfc4c1565d3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51d09b8a3c8b4e63838cd3c489766a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fd76d4fba649b2a993ee16939b95ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b55d4aa373dd40ab8b66991d6ad8f811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c9107b6955340c69a338ce950e4e6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinav1004/Colab-Notebooks/blob/main/test_llava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKOi2YWidQae",
        "outputId": "eb6954fa-37c0-4311-d005-9f607b7144a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "e9ae745755b44a2daf406c50d3161d6e",
            "3801450651da496fa5c5d3503d6a0cb9",
            "c7e57748afca439696ca942234070afd",
            "52ab3e64486c4ede84c6e15528ec9c12",
            "abd93eaeb1e94437a184cbf57aed5b9c",
            "4a97462338b7482e8478fa40dc4f622f",
            "f9cef04c59fc4f7ca5ddfc4c1565d3be",
            "51d09b8a3c8b4e63838cd3c489766a7a",
            "b6fd76d4fba649b2a993ee16939b95ae",
            "b55d4aa373dd40ab8b66991d6ad8f811",
            "0c9107b6955340c69a338ce950e4e6ac"
          ]
        },
        "id": "kwDAZ-4KY534",
        "outputId": "e12836ad-e9f9-48a8-ff8c-08b6cdbc2343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLaVA model: llava-hf/llava-1.5-7b-hf on device: cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9ae745755b44a2daf406c50d3161d6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1: Garbage Complaint (Prediction based on prompt + image content) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Text: The street looks awful today. This mess has been sitting here for three days! @LocalCityHall\n",
            "Prediction: **RELEVANT (Garbage Complaint)**\n",
            "Confidence: **43.58%**\n",
            "----------------------------------------\n",
            "\n",
            "--- Example 2: Non-Complaint Tweet (Prediction based on prompt + image content) ---\n",
            "Error loading image from URL: 403 Client Error: Forbidden for url: https://upload.wikimedia.org/wikipedia/commons/f/f1/Vuilnis_bij_Essent_Milieu.jpg\n",
            "Tweet Text: So thankful for this beautiful hike this morning. Nature is the best therapy!\n",
            "Prediction: **RELEVANT (Garbage Complaint)**\n",
            "Confidence: **57.69%**\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "import io\n",
        "import numpy as np # Needed for np.exp()\n",
        "\n",
        "# --- Configuration ---\n",
        "# The dedicated class for LLaVA (LlavaForConditionalGeneration) is often preferred\n",
        "# over the generic AutoModelForCausalLM/AutoModelForVision2Seq for stability.\n",
        "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.bfloat16\n",
        "\n",
        "# Recommended: Configuration for 4-bit quantization to save GPU VRAM\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=DTYPE,\n",
        "    # bnb_4bit_quant_type=\"nf4\", # Optional: can specify quant type\n",
        "    # bnb_4bit_use_double_quant=True, # Optional: can use double quant\n",
        ")\n",
        "\n",
        "# --- Helper Function for Image Loading ---\n",
        "def load_image_from_url(url: str) -> Image.Image:\n",
        "    \"\"\"Loads an image from a URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=10)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "        return image\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error loading image from URL: {e}\")\n",
        "        # Return a dummy black image if loading fails\n",
        "        return Image.new('RGB', (336, 336), color = 'black')\n",
        "\n",
        "# --- Step 1: Initialize the Processor and Model (Direct Loading) ---\n",
        "print(f\"Loading LLaVA model: {MODEL_ID} on device: {DEVICE}...\")\n",
        "\n",
        "# Use AutoProcessor for correct image and text preprocessing\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# CRITICAL FIX: Use LlavaForConditionalGeneration instead of AutoModelForCausalLM\n",
        "# This resolves the \"Unrecognized configuration class\" ValueError.\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=DTYPE # Set torch_dtype for the model itself\n",
        ")\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# --- Step 2: Define the Prediction Function (MODIFIED TO RETURN CONFIDENCE) ---\n",
        "\n",
        "def classify_tweet_relevance(image: Image.Image, tweet_text: str):\n",
        "    \"\"\"\n",
        "    Uses LLaVA in a zero-shot manner for binary classification, returning label and confidence.\n",
        "    \"\"\"\n",
        "    # CRITICAL: Prompt Engineering for Classification\n",
        "    # LLaVA requires the <image> token and a specific USER/ASSISTANT format.\n",
        "    classification_prompt = (\n",
        "        \"USER: <image>\\n\"\n",
        "        \"Analyze the visual content of the image and the following tweet text.\\n\"\n",
        "        \"The classification task is: Determine if this post is a **complaint regarding garbage, waste, or overflowing bins**.\\n\"\n",
        "        f\"Tweet: \\\"{tweet_text}\\\"\\n\"\n",
        "        \"Answer with only the single, capitalized word: 'RELEVANT' or 'NOT RELEVANT'.\\n\"\n",
        "        \"ASSISTANT:\"\n",
        "    )\n",
        "\n",
        "    # 1. Prepare inputs using the processor\n",
        "    inputs = processor(\n",
        "        text=classification_prompt,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE, DTYPE)\n",
        "\n",
        "    # 2. Generate the prediction, requesting scores for confidence calculation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id,\n",
        "            # --- LINES ADDED FOR CONFIDENCE ---\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    # 3. Process the output and confidence\n",
        "    output_ids = outputs.sequences\n",
        "    scores = outputs.scores\n",
        "\n",
        "    # Decode the generated text\n",
        "    input_len = inputs['input_ids'].shape[1]\n",
        "    generated_text = processor.decode(\n",
        "        output_ids[0, input_len:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip().upper()\n",
        "\n",
        "    # Calculate Confidence (Log-likelihood of the first generated token)\n",
        "    first_token_logits = scores[0].float()[0]\n",
        "\n",
        "    # Convert logits to log-probabilities\n",
        "    log_probs = torch.log_softmax(first_token_logits, dim=-1)\n",
        "\n",
        "    # Get the ID of the first *generated* token\n",
        "    predicted_token_id = output_ids[0, input_len].item()\n",
        "\n",
        "    # Get the log-probability of the generated token\n",
        "    log_confidence = log_probs[predicted_token_id].item()\n",
        "\n",
        "    # Convert log-probability to a percentage confidence score\n",
        "    confidence_score = float(np.exp(log_confidence) * 100)\n",
        "\n",
        "    # 4. Classify based on the generated text\n",
        "    if \"RELEVANT\" in generated_text:\n",
        "        label = \"RELEVANT (Garbage Complaint)\"\n",
        "    elif \"NOT RELEVANT\" in generated_text:\n",
        "        label = \"NOT RELEVANT (Other Topic)\"\n",
        "    else:\n",
        "        label = f\"UNCLEAR: {generated_text}\"\n",
        "\n",
        "    # Return both the label and the confidence score\n",
        "    return label, confidence_score\n",
        "\n",
        "# --- Step 3: Example Usage (MODIFIED TO HANDLE TWO RETURN VALUES) ---\n",
        "\n",
        "print(\"\\n--- Example 1: Garbage Complaint (Prediction based on prompt + image content) ---\")\n",
        "# Using a famous scenic view image, so the prediction should lean NOT RELEVANT\n",
        "image_url_1 = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
        "image_1 = load_image_from_url(image_url_1)\n",
        "tweet_text_1 = \"The street looks awful today. This mess has been sitting here for three days! @LocalCityHall\"\n",
        "\n",
        "prediction_1, confidence_1 = classify_tweet_relevance(image_1, tweet_text_1)\n",
        "print(f\"Tweet Text: {tweet_text_1}\")\n",
        "print(f\"Prediction: **{prediction_1}**\")\n",
        "print(f\"Confidence: **{confidence_1:.2f}%**\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n--- Example 2: Non-Complaint Tweet (Prediction based on prompt + image content) ---\")\n",
        "# Corrected image URL for a second example\n",
        "image_url_2 = \"https://upload.wikimedia.org/wikipedia/commons/f/f1/Vuilnis_bij_Essent_Milieu.jpg\"\n",
        "image_2 = load_image_from_url(image_url_2)\n",
        "tweet_text_2 = \"So thankful for this beautiful hike this morning. Nature is the best therapy!\"\n",
        "\n",
        "prediction_2, confidence_2 = classify_tweet_relevance(image_2, tweet_text_2)\n",
        "print(f\"Tweet Text: {tweet_text_2}\")\n",
        "print(f\"Prediction: **{prediction_2}**\")\n",
        "print(f\"Confidence: **{confidence_2:.2f}%**\")\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDo5de3kZCzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}